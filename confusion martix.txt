# ==========================================================
# 1️⃣ Install Libraries
# ==========================================================
!pip install -q transformers datasets scikit-learn

# ==========================================================
# 2️⃣ Import Libraries
# ==========================================================
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from datasets import load_dataset
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

# ==========================================================
# 3️⃣ Load Model
# ==========================================================
model_name = "google/flan-t5-base"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# ==========================================================
# 4️⃣ Load Dataset
# ==========================================================
dataset = load_dataset("imdb")
test_data = dataset["test"].select(range(100))  # small subset

# ==========================================================
# 5️⃣ Prompt Strategies
# ==========================================================

# Zero-Shot
def zero_shot_prompt(text):
    return f"""
Classify the sentiment as Positive or Negative.

Text: {text}
Sentiment:
"""

# Few-Shot
def few_shot_prompt(text):
    return f"""
Text: I loved this movie.
Sentiment: Positive

Text: This was terrible.
Sentiment: Negative

Text: {text}
Sentiment:
"""

# Chain-of-Thought
def cot_prompt(text):
    return f"""
Classify the sentiment.

Text: {text}

Think step by step and then give final answer as Positive or Negative.
"""

# ==========================================================
# 6️⃣ Prediction Function
# ==========================================================
def get_prediction(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)
    outputs = model.generate(**inputs, max_length=100)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True).lower()
    
    if "positive" in response:
        return 1
    elif "negative" in response:
        return 0
    else:
        return 0

# ==========================================================
# 7️⃣ Evaluation Function (NO GRAPH)
# ==========================================================
def evaluate(prompt_function, strategy_name):
    
    y_true = []
    y_pred = []
    
    for example in test_data:
        prompt = prompt_function(example["text"])
        prediction = get_prediction(prompt)
        
        y_true.append(example["label"])
        y_pred.append(prediction)
    
    # Metrics
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    cm = confusion_matrix(y_true, y_pred)
    
    # Print Results
    print("\n====================================")
    print(f"Strategy: {strategy_name}")
    print("====================================")
    print(f"Accuracy  : {accuracy:.4f}")
    print(f"Precision : {precision:.4f}")
    print(f"Recall    : {recall:.4f}")
    print(f"F1-Score  : {f1:.4f}")
    print("\nConfusion Matrix:")
    print(cm)

# ==========================================================
# 8️⃣ Run All Strategies
# ==========================================================
evaluate(zero_shot_prompt, "Zero-Shot")
evaluate(few_shot_prompt, "Few-Shot")
evaluate(cot_prompt, "Chain-of-Thought")
